{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlxcHIQPHAyx",
        "outputId": "1cba2c5b-a383-40ac-fdd4-75ecb0a16871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished\n",
            "Final Q-table:\n",
            "[[0.531441   0.59049    0.4782969  0.531441  ]\n",
            " [0.531441   0.         0.43046721 0.4782969 ]\n",
            " [0.4782969  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.59049    0.6561     0.         0.531441  ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.81       0.         0.43023689]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.6561     0.         0.729      0.59049   ]\n",
            " [0.6561     0.81       0.81       0.        ]\n",
            " [0.729      0.9        0.         0.729     ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.81       0.9        0.729     ]\n",
            " [0.81       0.9        1.         0.81      ]\n",
            " [0.         0.         0.         0.        ]]\n",
            "Average reward: 0.9\n"
          ]
        }
      ],
      "source": [
        "# Q-learning on FrozenLake\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym   # RL environments (like FrozenLake)\n",
        "import random\n",
        "\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "# \"is_slippery=False\" = no random slips\n",
        "\n",
        "n_states = env.observation_space.n   # number of possible states (tiles in the lake)\n",
        "n_actions = env.action_space.n       # number of possible actions (left, down, right, up)\n",
        "\n",
        "#  creating the Q-table\n",
        "Q = np.zeros((n_states, n_actions))\n",
        "# big table filled with 0s\n",
        "# rows = states (where the agent is)\n",
        "# columns = actions (what move the agent can do)\n",
        "# each cell will store how good it is to do this action in this state\n",
        "\n",
        "# setting the learning parameters\n",
        "alpha = 0.8   # learning rate: how fast we update knowledge\n",
        "gamma = 0.9   # discount factor: how much we care about the future\n",
        "epsilon = 0.1 # exploration rate: chance to pick random action\n",
        "episodes = 50000  # how many games (episodes() the agent will play\n",
        "\n",
        "# to track performance\n",
        "rewards_per_episode = []\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state, _ = env.reset()  # start new episode at beginning\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:  # keep moving until goal or hole\n",
        "        #choose action (epsilon-greedy)\n",
        "        if random.uniform(0,1) < epsilon:\n",
        "            action = env.action_space.sample()  # explore: random move\n",
        "        else:\n",
        "            action = np.argmax(Q[state])        # exploit: best known move\n",
        "\n",
        "        # take action in environment\n",
        "        next_state, reward, done, truncated, info = env.step(action) #gives us the state we ended up in after taking the action, the reward, wether the game ended\n",
        "\n",
        "        # update Q-table\n",
        "        old_value = Q[state, action]  # what we thought before\n",
        "        next_max = np.max(Q[next_state])  # best value in next state\n",
        "        new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
        "        Q[state, action] = new_value\n",
        "\n",
        "        # move to the next state\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    rewards_per_episode.append(total_reward)\n",
        "\n",
        "\n",
        "print(\"Training finished\")\n",
        "print(\"Final Q-table:\")\n",
        "print(Q)\n",
        "\n",
        "# Average reward over 100 episodes\n",
        "print(\"Average reward:\", np.mean(rewards_per_episode[-100:]))\n"
      ]
    }
  ]
}