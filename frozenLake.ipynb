{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nTC0y6KlErml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e57ea5b-0674-42dc-b320-5bef209ed60f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting state: 0\n",
            " Running a random episode\n",
            "Step 0: action=0, next_state=0, reward=0.0\n",
            "Step 1: action=0, next_state=0, reward=0.0\n",
            "Step 2: action=3, next_state=0, reward=0.0\n",
            "Step 3: action=1, next_state=4, reward=0.0\n",
            "Step 4: action=2, next_state=5, reward=0.0\n",
            "Episode ended.\n",
            "Total reward collected: 0.0\n"
          ]
        }
      ],
      "source": [
        "# FrozenLake Random Agent\n",
        "# Goal: understand how an MDP works (states, actions, rewards).\n",
        "# trying this before applying Q-learning.\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "#first we need to create the environment\n",
        "#FrozenLake-v1 is a 4x4 grid with:\n",
        "# S = Start, G = Goal, H = Hole (danger), F = Frozen (safe)\n",
        "#the agent must learn a safe path from S to G.\n",
        "#is_slippery=False removes randomness.\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "\n",
        "#reset environment\n",
        "state, info = env.reset()\n",
        "print(\"Starting state:\", state)  # Always 0 (top-left corner)\n",
        "\n",
        "#run one episode with random actions\n",
        "total_reward = 0\n",
        "print(\" Running a random episode\")\n",
        "\n",
        "for step in range(10):  # limit to 10 moves just for demo\n",
        "    action = env.action_space.sample()  # choose a random action (0–3)\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    print(f\"Step {step}: action={action}, next_state={next_state}, reward={reward}\")\n",
        "\n",
        "    total_reward += reward\n",
        "    state = next_state\n",
        "\n",
        "    if terminated or truncated:\n",
        "        print(\"Episode ended.\")\n",
        "        break\n",
        "\n",
        "print(\"Total reward collected:\", total_reward)\n",
        "\n",
        "#Reward is only 1 if we reach the Goal (G).\n",
        "#Otherwise, reward = 0 (walking or falling in a hole).\n",
        "#This exercise shows how MDPs work: each action leads to a new state,\n",
        "#and rewards depend on sequences, not one single choice (like in bandits).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To make it learn over time we’d need to store a Q-table (next step)"
      ],
      "metadata": {
        "id": "zbX16HbciMop"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}